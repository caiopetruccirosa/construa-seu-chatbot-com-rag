{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["iZK8wGvYEU05"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Geração de texto com LLMs\n","\n","Neste notebook, iremos mostrar como realizar geração de texto com LLMs, através da API do Groq, uma plataforma de provedora de inferência em LLMs em cloud."],"metadata":{"id":"Cur2Vtr-RAou"}},{"cell_type":"markdown","source":["# Instalação e importação de pacotes"],"metadata":{"id":"yZRrO4My-XF3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oT0qeaeyLwfj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3d45eef-7e85-4c4e-a312-5b8f09baaa87"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m102.4/103.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["!pip install -q groq\n","!pip install -q langchain==0.1.7\n","!pip install -q langchain-groq"]},{"cell_type":"markdown","source":["# Configurando a API do Groq"],"metadata":{"id":"TEdCqSIG-eNa"}},{"cell_type":"code","source":["from google.colab import userdata\n","\n","GROQ_API_KEY = userdata.get('GROQ_API_KEY')"],"metadata":{"id":"cjtkdzXy8IC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Usando a LLM para inferência"],"metadata":{"id":"37Qa7GRiEnDJ"}},{"cell_type":"code","source":["from langchain_groq import ChatGroq\n","\n","LLM_MODEL_NAME = \"llama3-70b-8192\"\n","LLM_TEMPERATURE = 0\n","\n","llm = ChatGroq(\n","    temperature=LLM_TEMPERATURE,\n","    model_name=LLM_MODEL_NAME,\n","    api_key=GROQ_API_KEY\n",")"],"metadata":{"id":"pP6Hnv7h8JXR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LLM Calling"],"metadata":{"id":"62vJxPt1EYD-"}},{"cell_type":"code","source":["llm.call_as_llm(\"What are you?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"jyxesby98neM","executionInfo":{"status":"ok","timestamp":1723088252503,"user_tz":180,"elapsed":961,"user":{"displayName":"Caio Petrucci Rosa","userId":"08188944895391760158"}},"outputId":"193d1779-b6b9-4348-c4fb-59c21f7cfcab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm a large language model, which means I've been trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions.\\n\\nI can be used to:\\n\\n* Answer questions on various topics, from science and history to entertainment and culture\\n* Generate text on a given topic or subject\\n* Translate text from one language to another\\n* Summarize long pieces of text into shorter, more digestible versions\\n* Even create stories, poems, or dialogues\\n\\nI'm constantly learning and improving, so please bear with me if I make any mistakes. I'm here to help and provide information to the best of my abilities!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## Invocation"],"metadata":{"id":"Oi413IX2EWea"}},{"cell_type":"code","source":["messages = [\n","    (\n","        \"system\",\n","        \"You are a helpful assistant that translates English to Portuguese. Translate the user sentence.\",\n","    ),\n","    (\"human\", \"I love programming.\"),\n","]\n","\n","response = llm.invoke(messages)\n","response.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"zsqxC2rq8pFl","executionInfo":{"status":"ok","timestamp":1723089201866,"user_tz":180,"elapsed":716,"user":{"displayName":"Caio Petrucci Rosa","userId":"08188944895391760158"}},"outputId":"ae4fd460-59e5-4165-b4d1-87c8b6bbf731"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Eu amo programar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## Chaining"],"metadata":{"id":"iZK8wGvYEU05"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\n","        \"system\",\n","        \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n","    ),\n","    (\"human\", \"{input}\"),\n","])\n","\n","chain = prompt | llm\n","\n","response = chain.invoke({\n","    \"input_language\": \"English\",\n","    \"output_language\": \"Portuguese\",\n","    \"input\": \"I love programming.\",\n","})\n","response.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"6wAddyOM9qUs","executionInfo":{"status":"ok","timestamp":1723089463251,"user_tz":180,"elapsed":652,"user":{"displayName":"Caio Petrucci Rosa","userId":"08188944895391760158"}},"outputId":"031b03ee-72dc-47fc-ddef-f4f7797e14c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Eu amo programar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["english = \"I love programming.\"\n","german = chain.invoke({ \"input_language\": \"English\", \"output_language\": \"German\", \"input\": english, })\n","portuguese = chain.invoke({ \"input_language\": \"German\", \"output_language\": \"Portuguese\", \"input\": german.content, })\n","\n","print(f'\"{english}\" -> \"{german.content}\" -> \"{portuguese.content}\"')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TC2D8-7GHn8o","executionInfo":{"status":"ok","timestamp":1723089400491,"user_tz":180,"elapsed":658,"user":{"displayName":"Caio Petrucci Rosa","userId":"08188944895391760158"}},"outputId":"b6f7b689-a3c5-45d9-e35e-fa1d0aeb913e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\"I love programming.\" -> \"Ich liebe das Programmieren.\" -> \"Eu amo programar.\"\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jbo0jo0eIFXI"},"execution_count":null,"outputs":[]}]}